# Lab P1-01: Implementa√ß√£o de Self-Attention

Este reposit√≥rio cont√©m a implementa√ß√£o do mecanismo de *Scaled Dot-Product Attention*, conforme descrito no artigo *"Attention Is All You Need"*. O c√≥digo foi desenvolvido inteiramente usando a biblioteca NumPy, sem depender de frameworks de Deep Learning (como Keras ou PyTorch).

## üóÇ Estrutura do Reposit√≥rio
* `attention.py`: Cont√©m a classe `ScaledDotProductAttention` e a implementa√ß√£o do Softmax.
* `test_attention.py`: Script para execu√ß√£o de testes unit√°rios b√°sicos e valida√ß√£o num√©rica.
* `README.md`: Documenta√ß√£o atual do projeto.

## üöÄ Como rodar o c√≥digo

1. Certifique-se de ter o Python instalado.
2. Instale o NumPy caso n√£o possua:
   ```bash
   pip install numpy
   ```
3. Execute o script de testes para validar a implementa√ß√£o:

  ```bash
   python test_attention.py
   ```



## üß† A Normaliza√ß√£o (Scaling Factor)

No c√°lculo do Attention, o produto escalar entre Q e Kt √© dividido pela raiz quadrada da dimens√£o das chaves, representada por ‚àödk.

Isso √© necess√°rio para evitar que os valores do produto escalar fiquem excessivamente grandes quando trabalhamos com altas dimens√µes. Valores muito altos empurrariam a fun√ß√£o Softmax para regi√µes extremas onde os gradientes s√£o muito pequenos (problema de *vanishing gradients*), prejudicando o aprendizado da rede. A divis√£o atua como um estabilizador num√©rico.

## üìä Exemplo de Input e Output Esperado

Ao rodar o script de teste, a valida√ß√£o ocorre com os seguintes dados:

* **Input (Q, K, V):** Matrizes aleat√≥rias de formato `(3, 4)`, representando 3 tokens com dimens√£o de embedding igual a 4.
* **Output (Matriz de Aten√ß√£o Resultante):** Formato `(3, 4)`, representando a nova proje√ß√£o ponderada dos tokens.
* **Pesos de Aten√ß√£o (Softmax):** Uma matriz intermedi√°ria de formato `(3, 3)`. A soma dos valores em cada linha dessa matriz √© validada para garantir que resulte exatamente em `1.0`.

## üìö Artigos como Refer√™ncias

1. [Understanding Softmax with Numpy](https://medium.com/@amit25173/understanding-softmax-with-numpy-b7273d8ab205)
2. [Understanding the Attention Mechanism: A simple implementation using Python and Numpy](https://medium.com/@christoschr97/understanding-the-attention-mechanism-a-simple-implementation-using-python-and-numpy-3f1feae13fb7)
